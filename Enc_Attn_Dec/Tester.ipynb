{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from sacrebleu import corpus_bleu, raw_corpus_bleu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "# import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_vi import *\n",
    "from model_new import *\n",
    "from evaluation_fs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMB_SIZE = 300\n",
    "HIDDEN_SIZE = 288\n",
    "EPOCH = 15\n",
    "\n",
    "teacher_forcing_ratio = 1\n",
    "learning_rate = 0.002\n",
    "print_every = 50\n",
    "val_every = 500\n",
    "\n",
    "nowpath = os.path.abspath('.')\n",
    "zh_en_dataDir = nowpath + '//dataset//iwsltzhen'\n",
    "vi_en_dataDir = nowpath + '//dataset//iwsltvien'\n",
    "emb_dataDir = nowpath + '//dataset//embedding'\n",
    "res_dataDir = nowpath + '//results'\n",
    "res_dataDir_vi = nowpath + '//results//vi_new_attn'\n",
    "res_dataDir_zh = nowpath + '//results//zh_new_attn'\n",
    "\n",
    "MAX_LEN_filter = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Read 133317 sentence pairs\n",
      "Counted words:\n",
      "vi 35530\n",
      "en 47862\n",
      "0.9387850011626424  remains after filter.\n",
      "Embedding_matrix shape is (100000, 300)\n",
      "Embedding Vocabualry size is 99982\n",
      "Size of the pretrained embedding matrix for  vi  is  (35530, 300)\n",
      "0.34171122994652403 words appear in the pretrained dataset\n",
      "Embedding_matrix shape is (100000, 300)\n",
      "Embedding Vocabualry size is 99996\n",
      "Size of the pretrained embedding matrix for  en  is  (47862, 300)\n",
      "0.6783669717103339 words appear in the pretrained dataset\n",
      "865280\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "train_en = []\n",
    "zh_en_reader(train_en, vi_en_dataDir + '//train.tok.en')\n",
    "train_zh = []\n",
    "zh_en_reader(train_zh, vi_en_dataDir + '//train.tok.vi')\n",
    "dev_zh = []\n",
    "zh_en_reader(dev_zh, vi_en_dataDir + '//dev.tok.vi')\n",
    "dev_en = []\n",
    "zh_en_reader(dev_en, vi_en_dataDir + '//dev.tok.en')\n",
    "test_zh = []\n",
    "zh_en_reader(test_zh, vi_en_dataDir + '//test.tok.vi')\n",
    "test_en = []\n",
    "zh_en_reader(test_en, vi_en_dataDir + '//test.tok.en')\n",
    "\n",
    "data_preprocessor(dev_zh)\n",
    "data_preprocessor(dev_en)\n",
    "data_preprocessor(train_en)\n",
    "data_preprocessor(train_zh)\n",
    "data_preprocessor(test_zh)\n",
    "data_preprocessor(test_en)\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData_zh_en_ensemble('vi', 'en', train_zh, train_en)\n",
    "input_train, output_train = ConvertSentence2numpy(input_lang, output_lang, pairs, UNK_token, EOS_token, MAX_LEN_filter)\n",
    "input_test, output_test = ConvertSentence2numpy(input_lang, output_lang, list(zip(test_zh, test_en)), UNK_token, EOS_token)\n",
    "input_dev, output_dev = ConvertSentence2numpy(input_lang, output_lang, list(zip(dev_zh, dev_en)), UNK_token, EOS_token)\n",
    "\n",
    "print(len(input_train)/len(train_zh), ' remains after filter.')\n",
    "pre_zh_emb_matrix = Load_pretrained_emb(100000, input_lang, emb_dataDir)\n",
    "pre_en_emb_matrix = Load_pretrained_emb(100000, output_lang, emb_dataDir)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "train_dataset = TransDataset(input_train, output_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = TransDataset(input_dev, output_dev)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_vi = TransDataset(input_test, output_test)\n",
    "test_loader_vi = torch.utils.data.DataLoader(dataset=test_dataset_vi,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_val = TransDataset(input_train, output_train)\n",
    "train_loader_val = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters_encoder: 11678520\n",
      "num_parameters_decoder: 29449278\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, EMB_SIZE, HIDDEN_SIZE, pre_zh_emb_matrix).to(device)\n",
    "decoder = BahdanauAttnDecoderRNN(EMB_SIZE, HIDDEN_SIZE, output_lang.n_words, pre_en_emb_matrix).to(device)\n",
    "print('num_parameters_encoder:', sum(p.numel() for p in encoder.parameters()))\n",
    "print('num_parameters_decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "encoder.load_state_dict(torch.load(res_dataDir_vi + \"//vi_tmp_encoder_batch_1207_300_288_10.pth\"))\n",
    "decoder.load_state_dict(torch.load(res_dataDir_vi + \"//vi_tmp_attn_decoder_batch_1207_300_288_10.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_BS10 = evaluate(val_loader, encoder, decoder, output_lang, K_BS=10)\n",
    "val_Bleu_BS10 = corpus_bleu(predict_sentences_BS10, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.1417093296604"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_BS10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i was a kid , i thought that the first thing was the best country on the world and i said , &quot; we &apos;ve got nothing to envy . &quot;',\n",
       " 'i was very proud of my country .',\n",
       " 'in school , we spent a lot of time to learn about the metals of the metals of the needle ii -- except not to learn a lot about the outside world , except the united states , korea and japan was our enemies .',\n",
       " 'although i &apos;ve been wondering what the outside world is , but i still think that i &apos;m going to live both my life in the same way , until all of a sudden change change .',\n",
       " 'when i was seven years ago , i witnessed the first time in my life in my life , but i was still thinking my life was completely normal .',\n",
       " 'my family wasn &apos;t poor , and i was still hungry .',\n",
       " 'but in 1995 , my mother brought home a letter from a grandmother from a sisters to her mother .',\n",
       " 'and she wrote : when you read these lines , both five of your family had no longer ever ever existed , because they had nothing to eat in two weeks .',\n",
       " 'all of the same lying on the floor , and our bodies arrived as death was very close .',\n",
       " 'i was shocked .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_BS15[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i was a little bit , i thought that the first thing was the best country on the world and i used to sing &quot; we don &apos;t have to envy . &quot; ',\n",
       " 'i was very proud of my country . ',\n",
       " 'in school , we spent a lot of time to learn about the kim &apos;s lives of the kim ii -- not learning about the african , except the united states , korea and japan was our enemies . ',\n",
       " 'although i &apos;ve been wondering , i &apos;ve been wondering how the outside world , but i still think i &apos;m going to live in the same way of being done in the same way , until all of the suddenly change . ',\n",
       " 'when i was seven years , i saw people in the first time in my life , but i was still thinking my life was completely normal . ',\n",
       " 'my family wasn &apos;t poor , and i was not going to be hungry . ',\n",
       " 'but in 1995 , my mother brought home a letter from a woman who was a grandmother to do with her mother . ',\n",
       " 'and in that one , she read these lines , both five of their life has no longer ever ever ever existed , because her house has been not any time to eat in two weeks . ',\n",
       " 'all the same ones are on the floor , and our bodies come into a very close and feel like death is very close . ',\n",
       " 'i was shocked . ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_noBS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "switcher = {'ge': lambda x, y: len(x) >= y,\n",
    "        'le': lambda x, y: len(x) <= y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate([0,1,2,3,4,5][2:], 2):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(inData, dataLoader, orglang, method = 'ge', thre = 100, start=1000):\n",
    "    switcher = {\n",
    "        'ge': lambda x, y: len(x) >= y,\n",
    "        'le': lambda x, y: len(x) <= y\n",
    "    }\n",
    "    reList = []\n",
    "    reLoList = []\n",
    "    reOrgList = []\n",
    "    indList = []\n",
    "    for i, data in enumerate(inData[start:], start):\n",
    "        if switcher[method](data, thre):\n",
    "            reList.append(data)\n",
    "            indList.append(i)\n",
    "        if len(reList) > 3:\n",
    "            break\n",
    "    for i, data in enumerate(dataLoader):\n",
    "        if i in indList:\n",
    "            reLoList.append(data)\n",
    "            reOrgList.append(orglang[i])\n",
    "    return(reList, reLoList, reOrgList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_long, test_loader_vi_long, test_org_vi_long = sampler(test_en, test_loader_vi, test_zh, method = 'ge', thre = 150, start=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_predict_sentences_long_BS4 = evaluate(test_loader_vi_long, encoder, decoder, output_lang, K_BS=4)\n",
    "t_Bleu_long_BS4_r = raw_corpus_bleu(t_predict_sentences_long_BS4, [test_en_long]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.729864200664572"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Bleu_long_BS4_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cười Nếu bạn nghĩ đến phim \" Avata \" nếu bạn nghĩ về việc mọi nó đã lay_động mọi người như thế_nào - - đừng để_ý đến câu_chuyện Pocahontas làm_gì - - Tại_sao lại bị lay_động bởi biểu_tượng đến_thế',\n",
       " 'Gần đây , tôi đã khảo_sát với hơn 2.000 người Mỹ , và trung_bình số lựa_chọn mà người châu mỹ điển_hình đã làm là khoảng 70 lần trong 1 ngày',\n",
       " 'Và những nhà khoa_học đã dẫn ra tất_cả những nhiệm_vụ khác_nhau mà những CEO đó tham_gia và họ đã tốn bao nhiều thời_gian để thực_hiện những quyết_định liên qua đén những nhiệm_vụ đó',\n",
       " 'Khi tôi là sinh_viên tốt_nghiệp từ đại_học Stanford , tôi đã từng vào rất nhiều cửa_hàng tạp_hoá mắc tiền , lúc đó , tôi thật_sự khá giàu ,']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org_vi_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you think about film , &quot; if you think about things that have moved people like -- don &apos;t pay attention to the story ?',\n",
       " 'recently , i &apos;ve been investigating over 2,000 americans , and the average number of choices that the american americans have done is about 70 times a day .',\n",
       " 'and the scientists have led all sorts of different tasks that ceos have been involved and they &apos;ve spent a lot of time to do these decisions .',\n",
       " 'when i was a graduate student from stanford university , i used to be a lot of grocery shops , at the time , i was pretty rich .']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_predict_sentences_long_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grammar mistakes, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And if you think about &quot; Avatar , &quot; if you think of why people were so touched by it -- never mind the Pocahontas story -- why so touched by the imagery ?',\n",
       " 'I recently did a survey with over 2,000 Americans , and the average number of choices that the typical American reports making is about 70 in a typical day .',\n",
       " 'And these scientists simply documented all the various tasks that these CEOs engaged in and how much time they spent engaging in making decisions related to these tasks .',\n",
       " 'So when I was a graduate student at Stanford University , I used to go to this very , very upscale grocery store ; at least at that time it was truly upscale .']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_short, test_loader_vi_short, test_org_vi_short = sampler(test_en, test_loader_vi, test_zh, method = 'le', thre = 40, start=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_predict_sentences_short_BS4 = evaluate(test_loader_vi_short, encoder, decoder, output_lang, K_BS=4)\n",
    "t_Bleu_short_BS4 = corpus_bleu(t_predict_sentences_short_BS4, [test_en_short]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.863110283992324"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Bleu_short_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cám_ơn rất nhiều',\n",
       " 'Bây_giờ , nghĩ về những sự lựa_chọn của chính bạn',\n",
       " 'Cảm_ơn .',\n",
       " \"Đây là cửa_hàng với tên Drager ' s\"]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org_vi_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you very much .',\n",
       " 'now , think about your own choices .',\n",
       " 'thank you .',\n",
       " 'this is the store with the name of the &quot; star . &quot;']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_predict_sentences_short_BS4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you very much .',\n",
       " 'Think about your own choices .',\n",
       " 'Thank you .',\n",
       " 'It was a store called Draeger &apos;s .']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.04698098099584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Bleu_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "t_predict_sentences_BS10 = evaluate(test_loader, encoder, decoder, output_lang, K_BS=10)\n",
    "t_Bleu_BS10 = corpus_bleu(t_predict_sentences_BS10, [test_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.87303338870817"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Bleu_BS10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences = evaluate(val_loader, encoder, decoder, output_lang, K_BS=4)\n",
    "val_Bleu = corpus_bleu(predict_sentences, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i was a little kid , i thought that the first thing was the best country on the world and i used to sing &quot; we don &apos;t have to envy . &quot;',\n",
       " 'i was very proud of my country .',\n",
       " 'in school , we spend a lot of time to learn about the kim &apos;s lives of the kim ii -- i &apos;m not going to learn a lot about the outside world , except the united states , korea and japan is our enemies .',\n",
       " 'although i &apos;ve been wondering , i &apos;ve been wondering where the outside world is , but i still think that i &apos;m going to live in the first day , until all of a sudden change change .',\n",
       " 'when i was seven years ago , i witnessed the first time in my life in my life , but i was still thinking my life in this is completely normal .',\n",
       " 'my family wasn &apos;t poor , and i was still hungry .',\n",
       " 'but in 1995 , my mother brought home a letter from a grandmother from a grandmother to her mother .',\n",
       " 'and in that writing : when you read these lines , both the family of her five friends had no longer ever ever existed in this two weeks .',\n",
       " 'all the same thing is on the floor , and our bodies come to be able to feel like death is very close .',\n",
       " 'i was shocked .']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.52298297858399"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;',\n",
       " 'And I was very proud .',\n",
       " 'In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .',\n",
       " 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .',\n",
       " 'When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .',\n",
       " 'My family was not poor , and myself , I had never experienced hunger .',\n",
       " 'But one day , in 1995 , my mom brought home a letter from a coworker &apos;s sister .',\n",
       " 'It read , &quot; When you read this , all five family members will not exist in this world , because we haven &apos;t eaten for the past two weeks .',\n",
       " 'We are lying on the floor together , and our bodies are so weak we are ready to die . &quot;',\n",
       " 'I was so shocked .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters_encoder: 12239544\n",
      "num_parameters_decoder: 35790654\n"
     ]
    }
   ],
   "source": [
    "encoder_1 = EncoderRNN(input_lang.n_words, EMB_SIZE, HIDDEN_SIZE, pre_zh_emb_matrix).to(device)\n",
    "decoder_1 = BahdanauAttnDecoderRNN(EMB_SIZE, HIDDEN_SIZE, output_lang.n_words, pre_en_emb_matrix).to(device)\n",
    "print('num_parameters_encoder:', sum(p.numel() for p in encoder.parameters()))\n",
    "print('num_parameters_decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "encoder_1.load_state_dict(torch.load(res_dataDir + \"//vi_a_encoder_batch_1205_t1300_384_20_2.pth\"))\n",
    "decoder_1.load_state_dict(torch.load(res_dataDir + \"//vi_a_attn_decoder_batch_1205_t1300_384_20_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_1 = evaluate(val_loader, encoder_1, decoder_1, output_lang)\n",
    "val_Bleu_1 = corpus_bleu(predict_sentences_1, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i think the , , i the the the , , , i i , i i i don don don &apos;t &apos;t &apos;t &apos;t don &apos;t &apos;t &apos;t &apos;t &apos;t . . ',\n",
       " 'i was proud proud proud proud proud . . ',\n",
       " 'in , , , , many , many to to we , , many many many , , , , , , , , , , , , , , , , , , , , , , , the the . . ',\n",
       " 'but i i knew thinking thinking , , , , , , , , , i , i , i , i i , i i , i life life life life . . ',\n",
       " 'when i first first first my life , , life , life , , , i i i i life life life life life life life life . . ',\n",
       " 'my family had poor and , , and i , never , never never . . ',\n",
       " 'but , mother mother , mother , mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother a mother mother . . . ',\n",
       " 'and in this , , , , , because , , , , , , because , , , , because , in in in in in in . . . . ',\n",
       " 'all of our our our , , , , , , , , , our our our our . . . ',\n",
       " 'i was shocked shocked . ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.55278820416851"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentences_2 = evaluate(val_loader, encoder_1, decoder_1, output_lang)\n",
    "val_Bleu_2 = corpus_bleu(predict_sentences_2, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1955601883853062"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentences_3 = evaluate([item for i, item in enumerate(train_loader_val) if i < 100], encoder_1, decoder_1, output_lang)\n",
    "val_Bleu_3 = corpus_bleu(predict_sentences_3, [train_en[:100]]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Bleu_3 = corpus_bleu(predict_sentences_3, [train_en[:100]]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45578048872221205"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the science science science science science . . ',\n",
       " 'i want to to you you you you you scientific you you you you the the the the the the on . . . ',\n",
       " 'there &apos;s like when when when when when when when when when when when when when when when when when when when when when climate and . . ',\n",
       " 'both are a scientific scientific science science science science . ',\n",
       " 'the headlines headlines this this , headlines the the , , , , , , , , the the the the the the the the the the the the their . . . ',\n",
       " 'scientists scientists written by by 40 40 . . . ',\n",
       " 'they wrote about 1,000 pages about 1,000 . . ',\n",
       " 'and all all of scientists are from and and and and from from reviewed reviewed from . . ',\n",
       " 'it &apos;s a big community , community community community , community community community community the the the the the the the the the the . . . ',\n",
       " 'every year , san san san francisco san francisco francisco . . ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2053637863838014"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_fs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1880997579653823"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_4 = evaluate(val_loader, encoder_1, decoder_1, output_lang)\n",
    "val_Bleu_4 = corpus_bleu(predict_sentences_4, [dev_en]).score\n",
    "val_Bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i think , the , i , the the i the i the i i i don don don &apos;t &apos;t &apos;t &apos;t &apos;t &apos;t &apos;t &apos;t &apos;t &apos;t . . &quot; <eos>',\n",
       " 'i was proud proud proud proud proud . . <eos>',\n",
       " 'in many , , many many many many of of , , , , to to , , , , , , , , , , , , , , , , , , , , , , , the . . <eos>',\n",
       " 'but i i knew thinking thinking , , , , , , , , , , , i , i , , i i , , i i i life life life life . . <eos>',\n",
       " 'when i i my first first , , life , my life life life life , , , , i i life life life life life . . <eos>',\n",
       " 'my family had poor poor , and poor , i never never never never . . <eos>',\n",
       " 'but , mother , mother , mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother mother . . <eos>',\n",
       " 'and in this this , , , , , , because , , , , , because , because because because , because in in in in in . . . . <eos>',\n",
       " 'all of our our our , , , , , , , , , , our our our . . <eos>',\n",
       " 'i was shocked shocked . <eos>']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0872736788851187e-20\n",
      "1.885156998846001e-20\n",
      "1.7544745128033867e-20\n",
      "6.995059144378444e-21\n"
     ]
    }
   ],
   "source": [
    "predict_sentences = evaluate(val_loader, encoder, decoder, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_6 = evaluate(val_loader, encoder, decoder, output_lang)\n",
    "val_Bleu_6 = corpus_bleu(predict_sentences_6, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i was a little , i i think that the best the the the the the and and i i love we we we we we &quot; &quot;',\n",
       " 'i was very proud about my country .',\n",
       " 'in school , we spent a lot of time to learn about the of of the , , , , not not lot about the outside the , , , , , , and and our',\n",
       " 'even though i asked myself to the the world , but but i i i i life life life life in , , , , all everything <pad>',\n",
       " 'when i was seven seven , i i see the the the the time first time , , but i my life life life life was <pad>',\n",
       " 'my family was poor , and i didn &apos;t have to hungry .',\n",
       " 'but in a day day , , mother mother took a a a of of a a to the to her . <pad>',\n",
       " 'and it was : : when you read these these family family family family not not , , , because because had no no in in <pad>',\n",
       " 'all all on the the , our our bodies , that that as as as .',\n",
       " 'i was shocked .']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_6[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.507029939562269"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try the best available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters_encoder: 11516088\n",
      "num_parameters_decoder: 28072766\n"
     ]
    }
   ],
   "source": [
    "encoder_1 = EncoderRNN(input_lang.n_words, EMB_SIZE, HIDDEN_SIZE, pre_zh_emb_matrix).to(device)\n",
    "decoder_1 = BahdanauAttnDecoderRNN(EMB_SIZE, HIDDEN_SIZE, output_lang.n_words, pre_en_emb_matrix).to(device)\n",
    "print('num_parameters_encoder:', sum(p.numel() for p in encoder.parameters()))\n",
    "print('num_parameters_decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "encoder_1.load_state_dict(torch.load(res_dataDir + \"//vi_tmp_encoder_batch_1206_t1300_256_10.pth\"))\n",
    "decoder_1.load_state_dict(torch.load(res_dataDir + \"//vi_tmp_attn_decoder_batch_1206_t1300_256_10.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/fs1520/NLP/project/model.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(output))\n",
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_7 = evaluate(val_loader, encoder_1, decoder_1, output_lang, K_BS=15)\n",
    "val_Bleu_7 = corpus_bleu(predict_sentences_7, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.03096524072704"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i got a child , i think that the best is the the the the the world and i i love we we we we we',\n",
       " 'i am very proud about my country .',\n",
       " 'in school , we spend a lot of time to learn about the the of the the , , , didn &apos;t &apos;t learn about the the , , , , , ,',\n",
       " 'even though i asked wonder , i didn &apos;t know the world outside , but i still think that i live alive in the world , until all of all of all .',\n",
       " 'when i was seven seven , i i saw the that the the the time first time time , , but but i my life life <pad>',\n",
       " 'my family wasn &apos;t poor , and i didn &apos;t have been hungry .',\n",
       " 'but on a day day , my mother mother took a a a a a a people the with',\n",
       " 'and it was : : when you read these these , my family family family not not so , , because because &apos;t had &apos;t <pad>',\n",
       " 'all all on the floor , and our body bodies have to feel like that is dying .',\n",
       " 'i was shocked .']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences_7[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_8 = evaluate(val_loader, encoder_1, decoder_1, output_lang, K_BS=6)\n",
    "val_Bleu_8 = corpus_bleu(predict_sentences_8, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8166175596524"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/fs1520/NLP/project/model.py:129: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(output))\n",
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_7_org = evaluate_org(val_loader, encoder_1, decoder_1, output_lang)\n",
    "val_Bleu_7_org = corpus_bleu(predict_sentences_7_org, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.33661623945737"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_7_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "predict_sentences_10 = evaluate(val_loader, encoder_1, decoder_1, output_lang, K_BS=10)\n",
    "val_Bleu_10 = corpus_bleu(predict_sentences_10, [dev_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.951006851903207"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Bleu_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder, output_lang, MAX_LENGTH = 100, K_BS=4):\n",
    "    def Beam_Search_Dec_Iterator(MAX_LENGTH, decoder_input, decoder_hidden, encoder_outputs, K_BS):\n",
    "    ## K_BS: select K max possible values in candidates\n",
    "        last_candidates = [[1, [decoder_input], decoder_hidden]]\n",
    "        out_candidates = []\n",
    "        min_proba = 1\n",
    "        for di in range(MAX_LENGTH):\n",
    "            cur_candidates = []\n",
    "            if len(last_candidates) <= 0:\n",
    "                break\n",
    "            for proba, cur_centense, decoder_hidden in last_candidates:\n",
    "                decoder_input = cur_centense[-1].detach()\n",
    "                # print(decoder_input)\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.to(device), decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.data.topk(K_BS)\n",
    "                # print([exp(x.item()) for x in topv[0]], topi)\n",
    "                for j, v in enumerate(topi[0]):\n",
    "                    # print()\n",
    "                    this_cen = cur_centense+[v.unsqueeze(0).unsqueeze(0)]\n",
    "                    # this_proba = proba*(exp(topv[0][j].item()))\n",
    "                    this_proba = proba*exp(topv[0][j].item())\n",
    "                    this_hidden = decoder_hidden\n",
    "                    if di == MAX_LENGTH-1:\n",
    "                        this_cen += [torch.tensor([[EOS_token]])]\n",
    "                        out_candidates.append([this_proba, this_cen])\n",
    "                    if v.item() == EOS_token or v.item() == PAD_token:\n",
    "                        this_cen += [torch.tensor([[EOS_token]])]\n",
    "                        out_candidates.append([this_proba, this_cen])\n",
    "                        # min_proba = min(min_proba, this_proba)\n",
    "                        # K_BS -= 1\n",
    "                    else:\n",
    "                        cur_candidates.append([this_proba, this_cen, this_hidden])\n",
    "            # print(cur_candidates[0])\n",
    "            cur_candidates = sorted(cur_candidates, key=lambda x:-x[0])[:K_BS]\n",
    "            last_candidates = cur_candidates[:K_BS]\n",
    "\n",
    "        out_candidates = sorted(out_candidates, key=lambda x:-x[0])[:K_BS]\n",
    "        out_sentences = []\n",
    "        for _, can in out_candidates:\n",
    "            # print(_)\n",
    "            out_sentences.append(' '.join([output_lang.index2word[v.item()] for v in can]).strip('SOS EOS'))\n",
    "\n",
    "        return(out_sentences[0])\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    sentences_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_sentence, output_sentence) in enumerate(loader):\n",
    "            encoder_hidden = encoder.initHidden(input_sentence.size()[0]).cuda()\n",
    "            encoder_outputs, encoder_hidden = encoder(input_sentence.to(device), encoder_hidden)\n",
    "\n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            #predicted_sentence = []\n",
    "            predicted_sentences = Beam_Search_Dec_Iterator(MAX_LENGTH, decoder_input, decoder_hidden, encoder_outputs, K_BS)\n",
    "\n",
    "            sentences_list.append(predicted_sentences.strip('<pad>'))\n",
    "\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Beam_Search_Dec_Iterator(MAX_LENGTH, decoder_input, decoder_hidden, encoder_outputs, K_BS):\n",
    "    ## K_BS: select K max possible values in candidates\n",
    "    last_candidates = [[1, [decoder_input], decoder_hidden]]\n",
    "    out_candidates = []\n",
    "    min_proba = 1\n",
    "    for di in range(MAX_LENGTH):\n",
    "        cur_candidates = []\n",
    "        if len(last_candidates) <= 0:\n",
    "            break\n",
    "        for proba, cur_centense, decoder_hidden in last_candidates:\n",
    "            decoder_input = cur_centense[-1].detach()\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.to(device), decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(K_BS)\n",
    "            for j, v in enumerate(topi):\n",
    "                this_cen = cur_centense+[v]\n",
    "                this_proba = proba*topv[j].item()\n",
    "                this_hidden = decoder_hidden\n",
    "                if di == MAX_LENGTH-1:\n",
    "                    this_cen += EOS_token\n",
    "                if v.item() == EOS_token or v.item() == PAD_token:\n",
    "                    this_cen += EOS_token\n",
    "                    out_candidates.append([this_proba, this_cen])\n",
    "                    min_proba = min(min_proba, this_proba)\n",
    "                    # K_BS -= 1\n",
    "                else:\n",
    "                    cur_candidates.append([this_proba, this_cen, this_hidden])\n",
    "        cur_candidates = sorted(cur_candidates, key=lambda x:-x[0])[:K_BS]\n",
    "        last_candidates = []\n",
    "        for cen in cur_candidates:\n",
    "            if cen[0] > min_proba:\n",
    "                if len(last_candidates) < K_BS:\n",
    "                    last_candidates.append(cen)\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "    out_candidates = sorted(out_candidates, key=lambda x:-x[0])[:K_BS]\n",
    "    out_sentences = []\n",
    "    for _, can in out_candidates:\n",
    "        out_sentences.append(' '.join([output_lang.index2word[v.item()] for v in can]))\n",
    "\n",
    "    return(out_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdfasfsf'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asdfasfsfs<eos>'.strip('<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_org(loader, encoder, decoder, output_lang, MAX_LENGTH = 100):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    sentences_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_sentence, output_sentence) in enumerate(loader):\n",
    "\n",
    "            encoder_hidden = encoder.initHidden(input_sentence.size()[0]).cuda()\n",
    "            encoder_outputs, encoder_hidden = encoder(input_sentence.to(device), encoder_hidden)\n",
    "\n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            #predicted_sentence = []\n",
    "            predicted_sentence = ''\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.to(device), decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token or topi.item() == PAD_token:\n",
    "                    # predicted_sentence += '<eos>'\n",
    "                    predicted_sentence += ''\n",
    "                    break\n",
    "                else:\n",
    "                    #predicted_sentence.append(output_lang.index2word[topi.item()])\n",
    "                    predicted_sentence += output_lang.index2word[topi.item()]\n",
    "                    predicted_sentence += \" \"\n",
    "\n",
    "                decoder_input = topi.detach()\n",
    "                #decoder_input = decoder_input.unsqueeze(0)\n",
    "            if di==MAX_LENGTH -1:\n",
    "                # predicted_sentence += '<eos>'\n",
    "                predicted_sentence += ''\n",
    "\n",
    "            sentences_list.append(predicted_sentence.strip('SOS EOS'))\n",
    "            #decoder_attentions_list.append(decoder_attentions[:di + 1])\n",
    "\n",
    "        #return decoded_words_list, decoder_attentions_list\n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdfs'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'SOS asdfse EO'\n",
    "a.strip('SOS eEOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",'wow wow wow wow i am not shabi','wow wow wow i am so tai niubi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe = torch.Tensor([0.1,0.2,0.5,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/fs1520/pytorch_gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "eoe = F.softmax(zoe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2127,  0.2350,  0.3173,  0.2350])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "cce = eoe.data.topk(3)[1]\n",
    "for i in cce:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0, 1], [2, 1, 1], [1, 2, 3]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([[1,2,3],[2,1,1],[3,0,1]], key=lambda x:-x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aces = []\n",
    "aces.append('wow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aces.append('wowowow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'wowowow']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = {1:1,2:2,3:3}\n",
    "for i in list(ac.keys()):\n",
    "    if i != 2:\n",
    "        ac.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model_new_simple import *\n",
    "from evaluation_fs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "Read 213377 sentence pairs\n",
      "Counted words:\n",
      "zh 88426\n",
      "en 60699\n",
      "0.9515880343242242  remains after filter.\n",
      "Embedding_matrix shape is (100000, 300)\n",
      "Embedding Vocabualry size is 99994\n",
      "Size of the pretrained embedding matrix for  zh  is  (88426, 300)\n",
      "0.3373894555899849 words appear in the pretrained dataset\n",
      "Embedding_matrix shape is (100000, 300)\n",
      "Embedding Vocabualry size is 99996\n",
      "Size of the pretrained embedding matrix for  en  is  (60699, 300)\n",
      "0.6207186279839866 words appear in the pretrained dataset\n",
      "165764096\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "train_en = []\n",
    "zh_en_reader(train_en, zh_en_dataDir + '//train.tok.en')\n",
    "train_zh = []\n",
    "zh_en_reader(train_zh, zh_en_dataDir + '//train.tok.zh')\n",
    "dev_zh = []\n",
    "zh_en_reader(dev_zh, zh_en_dataDir + '//dev.tok.zh')\n",
    "dev_en = []\n",
    "zh_en_reader(dev_en, zh_en_dataDir + '//dev.tok.en')\n",
    "test_zh = []\n",
    "zh_en_reader(test_zh, zh_en_dataDir + '//test.tok.zh')\n",
    "test_en = []\n",
    "zh_en_reader(test_en, zh_en_dataDir + '//test.tok.en')\n",
    "\n",
    "data_preprocessor(dev_zh)\n",
    "data_preprocessor(dev_en)\n",
    "data_preprocessor(train_en)\n",
    "data_preprocessor(train_zh)\n",
    "data_preprocessor(test_zh)\n",
    "data_preprocessor(test_en)\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData_zh_en_ensemble('zh', 'en', train_zh, train_en)\n",
    "input_train, output_train = ConvertSentence2numpy(input_lang, output_lang, pairs, UNK_token, EOS_token, MAX_LEN_filter)\n",
    "input_test, output_test = ConvertSentence2numpy(input_lang, output_lang, list(zip(test_zh, test_en)), UNK_token, EOS_token)\n",
    "input_dev, output_dev = ConvertSentence2numpy(input_lang, output_lang, list(zip(dev_zh, dev_en)), UNK_token, EOS_token)\n",
    "\n",
    "print(len(input_train)/len(train_zh), ' remains after filter.')\n",
    "pre_zh_emb_matrix = Load_pretrained_emb(100000, input_lang, emb_dataDir)\n",
    "pre_en_emb_matrix = Load_pretrained_emb(100000, output_lang, emb_dataDir)\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "train_dataset = TransDataset(input_train, output_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = TransDataset(input_dev, output_dev)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_zh = TransDataset(input_test, output_test)\n",
    "test_loader_zh = torch.utils.data.DataLoader(dataset=test_dataset_zh,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_val = TransDataset(input_train, output_train)\n",
    "train_loader_val = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE=300\n",
    "HIDDEN_SIZE=384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88426"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters_encoder: 28108344\n",
      "num_parameters_decoder: 43551423\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, EMB_SIZE, HIDDEN_SIZE, pre_zh_emb_matrix).to(device)\n",
    "decoder = BahdanauAttnDecoderRNN(EMB_SIZE, HIDDEN_SIZE, output_lang.n_words, pre_en_emb_matrix).to(device)\n",
    "print('num_parameters_encoder:', sum(p.numel() for p in encoder.parameters()))\n",
    "print('num_parameters_decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "encoder.load_state_dict(torch.load(res_dataDir_zh + \"//zh_tmp_encoder_1210_simple_300_384_15.pth\"))\n",
    "decoder.load_state_dict(torch.load(res_dataDir_zh + \"//zh_tmp_attn_decoder_1210_simple_300_384_15.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_long, test_loader_zh_long, test_org_zh_long = sampler(test_en, test_loader_zh, test_zh, method = 'ge', thre = 150, start=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_predict_sentences_long_BS4 = evaluate(test_loader_zh_long, encoder, decoder, output_lang, K_BS=4)\n",
    "zh_Bleu_long_BS4 = raw_corpus_bleu(zh_predict_sentences_long_BS4, [test_en_long]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.821592547308882"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_Bleu_long_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这 上面 说 本 飞机 有 很多 精密 部件 协作 保障 你 飞行 安全 这 是不是 不是 让 你 很 有 信心',\n",
       " '我们 已经 体会 到 作为 一个 老师 你 的话 是 有 影响 影响力 的 如果 你 是 个 很 慈善 的 老师 你 的 教导 就 格外 意味 意味深长 深长',\n",
       " '我 记得 当 我 八九 八九岁 九岁 时 的 一次 我 早上 醒来 跑 到 客厅 所有 的 表兄 表兄妹 兄妹 都 在',\n",
       " '大概 十五 还是 二十 二十分 二十分钟 十分 十分钟 分钟 之后 她 站起 起来 穿过 过房 房间 牵住 我 的 手 对 我 说 过来 布莱恩 莱恩 我们 得 谈谈']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org_zh_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nd it says , &quot; well , ben plane has a lot of sophisticated parts , collaborative security , and you can fly , and you ',\n",
       " 'nd we &apos;ve experienced that as a teacher , you have a influence , and if you &apos;re a great teacher , you teach it , you ',\n",
       " 'nd i remember , when i was nine years old , i woke up to the living room and i woke up the living room with my life , and the brother brother ',\n",
       " 'it &apos;s about 15 minutes later , and after 10 minutes , she stood up through the room , and i said , &quot; brian , we &apos;ve got to ']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_predict_sentences_long_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It says , &apos; This plane has hundreds of thousands of tiny parts working together to make you a safe flight . &apos; Doesn &apos;t that make you feel confident ? &quot;',\n",
       " 'And I think what we &apos;ve learned is that , if you &apos;re a teacher your words can be meaningful , but if you &apos;re a compassionate teacher , they can be especially meaningful .',\n",
       " 'And I remember , when I was about eight or nine years old , waking up one morning , going into the living room , and all of my cousins were running around .',\n",
       " 'And after about 15 or 20 minutes of this , she got up and she came across the room and she took me by the hand and she said , &quot; Come on , Bryan . You and I are going to have a talk . &quot;']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_short, test_loader_zh_short, test_org_zh_short = sampler(test_en, test_loader_zh, test_zh, method = 'le', thre = 40, start=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_predict_sentences_short_BS4 = evaluate(test_loader_zh_short, encoder, decoder, output_lang, K_BS=4)\n",
    "zh_Bleu_short_BS4 = raw_corpus_bleu(zh_predict_sentences_short_BS4, [test_en_short]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.43890828734302"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_Bleu_short_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['所以 它 是 自我 回馈 的', '我们 正在 起飞', '我 妈妈 是 十个 个里 最 年轻 的', '我 永远 忘不掉 不掉']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org_zh_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so it &apos;s self .',\n",
       " 'we &apos;re taking off .',\n",
       " 'my mom was 10 .',\n",
       " 'i &apos;ll never forget that .']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_predict_sentences_short_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So it &apos;s feeding back on itself .',\n",
       " 'We &apos;re taking off .',\n",
       " 'My mom was the youngest of her 10 kids .',\n",
       " 'I never will forget it .']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters_encoder: 28108344\n",
      "num_parameters_decoder: 43551423\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, EMB_SIZE, HIDDEN_SIZE, pre_zh_emb_matrix).to(device)\n",
    "decoder = BahdanauAttnDecoderRNN(EMB_SIZE, HIDDEN_SIZE, output_lang.n_words, pre_en_emb_matrix).to(device)\n",
    "print('num_parameters_encoder:', sum(p.numel() for p in encoder.parameters()))\n",
    "print('num_parameters_decoder:', sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "encoder.load_state_dict(torch.load(res_dataDir_zh + \"//zh_tmp_encoder_1210_simple_t2_300_384_15.pth\"))\n",
    "decoder.load_state_dict(torch.load(res_dataDir_zh + \"//zh_tmp_attn_decoder_1210_simple_t2_300_384_15.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/fs1520/NLP/project/model_new_simple.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.out(output))\n",
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "zh_predict_sentences_BS4 = evaluate(test_loader, encoder, decoder, output_lang, K_BS=4)\n",
    "zh_Bleu_BS4 = corpus_bleu(zh_predict_sentences_BS4, [test_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.967703355055011"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_Bleu_BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi , hi . i &apos;m kevin . i &apos;m going to analyze managers on youtube , and my profession is watching youtube .',\n",
       " 'this is true .',\n",
       " 'so today , we &apos;re going to talk about why we &apos;re going <pad>',\n",
       " 'we all wanted to be a star celebrity singer , and when i was young , it was very tough to do .',\n",
       " 'but now , the internet now allows it to come true , so we can create the idea of doing things , and then the moment of success became part of the world .',\n",
       " 'any of you can be famous in the internet , in',\n",
       " 'but there &apos;s over 48 hours of video uploaded to youtube .',\n",
       " 'in many times , there &apos;s only a lot of numbers , and you see , people are people and <pad>',\n",
       " 'how did this happen ?',\n",
       " 'the three keys were opening up the community , engaged , and really unexpected .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_predict_sentences_BS4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    }
   ],
   "source": [
    "zh_predict_sentences_BS8 = evaluate(test_loader, encoder, decoder, output_lang, K_BS=8)\n",
    "zh_Bleu_BS8 = corpus_bleu(zh_predict_sentences_BS8, [test_en]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.80452819442737"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_Bleu_BS8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
